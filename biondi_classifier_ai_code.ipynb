{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pylab\n",
    "import tensorflow as tf\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 10 03:04:51 2019       \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| NVIDIA-SMI 384.130                Driver Version: 384.130                   |\r\n",
      "|-------------------------------+----------------------+----------------------+\r\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\r\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\r\n",
      "|===============================+======================+======================|\r\n",
      "|   0  TITAN Xp            Off  | 00000000:01:00.0  On |                  N/A |\r\n",
      "| 23%   34C    P5    21W / 250W |    457MiB / 12186MiB |      1%      Default |\r\n",
      "+-------------------------------+----------------------+----------------------+\r\n",
      "                                                                               \r\n",
      "+-----------------------------------------------------------------------------+\r\n",
      "| Processes:                                                       GPU Memory |\r\n",
      "|  GPU       PID   Type   Process name                             Usage      |\r\n",
      "|=============================================================================|\r\n",
      "+-----------------------------------------------------------------------------+\r\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[name: \"/device:CPU:0\"\n",
      "device_type: \"CPU\"\n",
      "memory_limit: 268435456\n",
      "locality {\n",
      "}\n",
      "incarnation: 17118137069235598313\n",
      ", name: \"/device:GPU:0\"\n",
      "device_type: \"GPU\"\n",
      "memory_limit: 11453762765\n",
      "locality {\n",
      "  bus_id: 1\n",
      "  links {\n",
      "  }\n",
      "}\n",
      "incarnation: 7011968063162370535\n",
      "physical_device_desc: \"device: 0, name: TITAN Xp, pci bus id: 0000:01:00.0, compute capability: 6.1\"\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "print(device_lib.list_local_devices())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/home/monuki/Documents/AI_code/CropPadTest5buff\n"
     ]
    }
   ],
   "source": [
    "cd /home/monuki/Documents/AI_code/CropPadTest5buff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0test  Book2.csv\t     labels.npy  stacks\t\t   UCI-43-18 raw.tif\r\n",
      "1test  CropPadTest5Buff.npy  MIPs\t UCI-43-18_C3.tif\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(802, 64, 64, 64, 3)\n",
      "(802, 1)\n"
     ]
    }
   ],
   "source": [
    "x = np.load('CropPadTest5Buff.npy')\n",
    "y = np.load('labels.npy')\n",
    "print(x.shape)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(?, 2)\n",
      "()\n"
     ]
    }
   ],
   "source": [
    "def create_model():\n",
    "    \"\"\"\n",
    "    Method to create the following CNN architecture:\n",
    "    \n",
    "      - BLOCK 1 (filter depth of 8) 64\n",
    "      - BLOCK 2 (filter depth of 16) 32\n",
    "      - BLOCK 3 (filter depth of 32) 16\n",
    "      - BLOCK 4 (filter depth of 64) 8\n",
    "      - RESHAPE\n",
    "      - HIDDEN LAYER (128 nodes)\n",
    "      - LOGIT SCORES (10 nodes)\n",
    "      \n",
    "    \"\"\"\n",
    "    # Reset our graph to build a new one\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Define placeholders for our images and labels\n",
    "    # ------------------------------------------------------------------------\n",
    "    #\n",
    "    # 1. Define images\n",
    "    # \n",
    "    #  - type: float32\n",
    "    #  - size: [None, 32, 32, 3] so that we can feed in as many images as we need\n",
    "    # \n",
    "    # 2. Define labels\n",
    "    # \n",
    "    #  - type: int64\n",
    "    #  - size: [None] so that we can feed in as many labels as we need\n",
    "    # \n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    im = tf.placeholder(tf.float32, [None, 64, 64, 64, 3])\n",
    "    #how to deal with 3d images????\n",
    "    labels = tf.placeholder(tf.int64, [None])\n",
    "    #labels = tf.placeholder(tf.int64, )\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Define convolutional blocks \n",
    "    # ------------------------------------------------------------------------\n",
    "    #\n",
    "    # As in previous assignments, we will use the tf.get_variables(...) method\n",
    "    # to create matrix variables initialized to random values.\n",
    "    # \n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # Block 1 (8 filter depth) 64^3 input\n",
    "    w1 = tf.get_variable('w1', [3, 3, 3, 3, 2], dtype=tf.float32)\n",
    "    w2 = tf.get_variable('w2', [3, 3, 3, 2, 2], dtype=tf.float32)\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(im, w1, strides=[1, 1, 1, 1, 1], padding='SAME'))\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w2, strides=[1, 2, 2, 2, 1], padding='SAME'))\n",
    "\n",
    "    # Block 2 (16 filter depth) 32^3 input\n",
    "    w3 = tf.get_variable('w3', [3, 3, 3, 2, 4], dtype=tf.float32)\n",
    "    w4 = tf.get_variable('w4', [3, 3, 3, 4, 4], dtype=tf.float32)\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w3, strides=[1, 1, 1, 1, 1], padding='SAME'))\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w4, strides=[1, 2, 2, 2, 1], padding='SAME'))\n",
    "    \n",
    "    # Block 3 (32 filter depth) 16^3 input\n",
    "    w5 = tf.get_variable('w5', [3, 3, 3, 4, 8], dtype=tf.float32)\n",
    "    w6 = tf.get_variable('w6', [3, 3, 3, 8, 8], dtype=tf.float32)\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w5, strides=[1, 1, 1, 1, 1], padding='SAME'))\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w6, strides=[1, 2, 2, 2, 1], padding='SAME'))\n",
    "    \n",
    "    # Block 4 (64 filter depth) 8^3 input\n",
    "    w7 = tf.get_variable('w7', [3, 3, 3, 8, 16], dtype=tf.float32)\n",
    "    w8 = tf.get_variable('w8', [3, 3, 3, 16, 16], dtype=tf.float32)\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w7, strides=[1, 1, 1, 1, 1], padding='SAME'))\n",
    "    layer = tf.nn.relu(tf.nn.conv3d(layer, w8, strides=[1, 2, 2, 2, 1], padding='SAME'))\n",
    "    \n",
    "    \n",
    "    # ------------------------------------------------------------------------\n",
    "    # Reshape to 1D vector \n",
    "    # ------------------------------------------------------------------------\n",
    "    \n",
    "    flattened = tf.reshape(layer, [-1, 4 * 4 * 4 * 16])\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Define our matmul operations\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    # Hidden layer (128 nodes)\n",
    "    ### got rid of based on advise from peter\n",
    "    \n",
    "    #w9 = tf.get_variable('w9', [4 * 4 * 4 * 512, 128], dtype=tf.float32)\n",
    "    #h1 = tf.nn.relu(tf.matmul(flattened, w9))\n",
    "    \n",
    "    # Logits (2 nodes)\n",
    "    w9 = tf.get_variable('w9', [4 * 4 * 4 * 16, 2], dtype=tf.float32)\n",
    "    logits = tf.matmul(flattened, w9)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Define our softmax cross-entropy loss\n",
    "    # ------------------------------------------------------------------------\n",
    "    # \n",
    "    # HINT: use tf.losses.sparse_softmax_cross_entropy() as above\n",
    "    #\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    loss = tf.losses.sparse_softmax_cross_entropy(labels=labels, logits=logits)\n",
    "\n",
    "    # ------------------------------------------------------------------------\n",
    "    # Define our optimizer\n",
    "    # ------------------------------------------------------------------------\n",
    "\n",
    "    train_op = tf.train.AdamOptimizer(0.001).minimize(loss)\n",
    "    \n",
    "    return im, labels, [w1, w2, w3, w4, w5, w6, w7, w8, w9], logits, loss, train_op\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Test our model\n",
    "# ------------------------------------------------------------------------\n",
    "# \n",
    "# If the graph was defined properly, we should be able to check the out\n",
    "# what the model outputs should look like. Can you guess by the shapes\n",
    "# of our logits and losses will be?\n",
    "# \n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "im, labels, weights, logits, loss, train_op = create_model()\n",
    "print(logits.shape)\n",
    "print(loss.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Create our model\n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "im, labels, weights, logits, loss, train_op = create_model()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Add to collections\n",
    "# ------------------------------------------------------------------------\n",
    "# \n",
    "# Collections are used by TensorFlow to keep track of certain intermediate \n",
    "# values for quick access during save/load functions.\n",
    "# \n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "tf.add_to_collection('im', im)\n",
    "tf.add_to_collection('logits', logits)\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Initialize our test graph\n",
    "# ------------------------------------------------------------------------\n",
    "# \n",
    "# What two things do we need to initialize our graph?\n",
    "# \n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "sess = tf.InteractiveSession()\n",
    "tf.global_variables_initializer().run()\n",
    "\n",
    "# ------------------------------------------------------------------------\n",
    "# Initialize our test graph\n",
    "# ------------------------------------------------------------------------\n",
    "# \n",
    "# Initialize a Saver object\n",
    "# \n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------------------------------------------------\n",
    "# Train our algorithm \n",
    "# ------------------------------------------------------------------------\n",
    "# \n",
    "# Let's set up a loop to train our algorithm by feeding it data iteratively.\n",
    "# For each iteration, we will feed a batch_size number of images into our \n",
    "# model and let it readjust it's neuronal weights.\n",
    "# \n",
    "# ------------------------------------------------------------------------\n",
    "\n",
    "def train_model(iterations=10, batch_size=4):\n",
    "    \n",
    "    accuracies = []\n",
    "    losses = []\n",
    "\n",
    "    for i in range(iterations):\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Grab a total of batch_size number of random images and labels \n",
    "        # --------------------------------------------------------------------\n",
    "        # \n",
    "        # 1. Pick batch_size number of random indices between 0 and 50,000\n",
    "        # 2. Select those images / labels\n",
    "        #\n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        rand_indices = np.random.randint(802, size=(batch_size))\n",
    "        x_batch = x[rand_indices]\n",
    "        y_batch = y[rand_indices]\n",
    "        y_batch = y_batch[..., 0]\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Normalize x_batch\n",
    "        # --------------------------------------------------------------------\n",
    "        # \n",
    "        # Currently, values in x range from 0 to 255. If we normalize these values\n",
    "        # to a mean of 0 and SD of 1 we will improve the stability of training\n",
    "        # and furthermore improve interpretation of learned weights. Use the\n",
    "        # following code to normalize your batch:\n",
    "        # \n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        x_batch = (x_batch - np.mean(x_batch)) / np.std(x_batch)\n",
    "\n",
    "        # Convert to types matching our defined placeholders\n",
    "        x_batch = x_batch.astype('float32')\n",
    "        y_batch = y_batch.astype('int64')\n",
    "\n",
    "        # Prepare feed_dict\n",
    "        feed_dict = {im: x_batch, labels: y_batch}\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Run training iteration via sess.run()\n",
    "        # --------------------------------------------------------------------\n",
    "        # \n",
    "        # Here, in addition to whichever ouputs we wish to extract, we need to\n",
    "        # also include the train_op variable. Including train_op will tell \n",
    "        # Tensorflow that in addition to calculating the intermediates of our graph,\n",
    "        # we also need to readjust the variables so that the overall loss goes\n",
    "        # down.\n",
    "        # \n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        outputs = sess.run([logits, loss, train_op], feed_dict)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Use argmax to determine highest logit (model guess)\n",
    "        # --------------------------------------------------------------------\n",
    "        # \n",
    "        # Keep in mind our logits matrix is (batch_size x 10) in size representing\n",
    "        # a total of batch_size number of predictions. How do we process this matrix\n",
    "        # with the np.argmax() to find the highest logit along each row of the matrix\n",
    "        # (e.g. find the prediction for each of our images)?\n",
    "        # \n",
    "        # HINT: what does the axis parameter in np.argmax(a, axis) specify?\n",
    "        # \n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        predictions = np.argmax(outputs[0], axis=1)\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Calculate accuracy \n",
    "        # --------------------------------------------------------------------\n",
    "        # \n",
    "        # Consider the following:\n",
    "        # \n",
    "        # - predictions = the predicted digits\n",
    "        # - y_batch = the ground-truth digits\n",
    "        # \n",
    "        # How do I calculate an accuracy % with this data?\n",
    "        # \n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        accuracy = np.sum(predictions == y_batch) / batch_size\n",
    "\n",
    "        # --------------------------------------------------------------------\n",
    "        # Accumulate and print iteration, loss and accuracy \n",
    "        # --------------------------------------------------------------------\n",
    "\n",
    "        print('Iteration %05i | Loss = %07.3f | Accuracy = %0.4f' %\n",
    "            (i + 1, outputs[1], accuracy))\n",
    "\n",
    "        losses.append(outputs[1])\n",
    "        accuracies.append(accuracy)\n",
    "        \n",
    "    return losses, accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------------------------------------------------------\n",
    "# Train model\n",
    "# --------------------------------------------------------------------\n",
    "losses, accuracies = train_model(iterations=10, batch_size=4)\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Graph outputs and accuracy\n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "import pylab\n",
    "pylab.plot(losses)\n",
    "pylab.title('Model loss over time')\n",
    "pylab.show()\n",
    "\n",
    "pylab.plot(accuracies)\n",
    "pylab.title('Model accuracy over time')\n",
    "pylab.show()\n",
    "\n",
    "# --------------------------------------------------------------------\n",
    "# Save model\n",
    "# --------------------------------------------------------------------\n",
    "# \n",
    "# In this step, all model variables and the underlying graph structure\n",
    "# are saved so that they can be reloaded. Although it looks like just one\n",
    "# file is saved here, in fact both a *.cpkt and *.cpkt.meta file are both\n",
    "# saved in this single line of code.\n",
    "#  \n",
    "# --------------------------------------------------------------------\n",
    "\n",
    "import os\n",
    "model_file = './model_cnn3d_test/model.ckpt'\n",
    "os.makedirs(os.path.dirname(model_file), exist_ok=True)\n",
    "print('Saving model')\n",
    "saver.save(sess, model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
